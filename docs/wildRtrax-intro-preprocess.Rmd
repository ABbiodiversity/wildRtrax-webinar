---
title: "wildRtrax"
subtitle: "An R package for environmental sensor data"
author: Marcus Becker, Alex MacPhail, Elly Knight 
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts] 
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      hightlightSpans: true
      countIncrementalSlides: false
    seal: true
---

```{r setup, include = FALSE, eval = TRUE}

options(htmltools.dir.version = FALSE)

library(knitr)
library(wildRtrax)
library(tidyverse)

opts_chunk$set(
  #prompt = T,
  fig.align="center", #fig.width=6, fig.height=4.5, 
  # out.width="748px", #out.length="520.75px",
  dpi=300, #fig.path='Figs/',
  cache=T#, echo=F, warning=F, message=F
  )

hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})

```

```{css, echo=FALSE}
/* custom.css */
.left-code {
  color: #777;
  width: 90%;
  height: 92%;
  float: left;
}
.left-code-less {
  color: #777;
  width: 90%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 58%;
  float: right;
  padding-left: 1%;
}
.right-plot-more {
  width: 65%;
  float: right;
  padding-left: 1%;
}
.plot-callout {
  height: 225px;
  width: 450px;
  bottom: 5%;
  right: 5%;
  position: absolute;
  padding: 0px;
  z-index: 100;
}
.plot-callout img {
  width: 100%;
  border: 4px solid #23373B;
}
body, h1, h2, h3, h4, h5, h6, p, ul, ol {
  font-family: "Agenda", sans-serif; font-size: 20px /* Replace "Agenda" with the actual font name */
}
.my-one-page-font {
  font-size: 18px;
}
.text-container {
            width: 300px; /* Set the width to your desired value in pixels or other units */
            margin: 0 auto; /* Center the container horizontally */
        }
.main-container { width: 1800px; max-width:2800px;}
.title-slide {
  background-image: url(hex-logo-pipit.png);
  background-position: 100% 0%;
  background-size: 400px;
  padding-left: 100px;  /* delete this for 4:3 aspect ratio */
}

```

# Preface

- We assume you're using an *environmental sensor* such as an **autonomous recording unit (ARU)** or **remote camera**


- We assume you know *WildTrax*

  - A web-enabled portal designed to manage, store, process, share and discover environmental sensor data


- We assume you know *R*

  - R is a programming language mainly used for statistical computing and data analysis
  
---

## What is `wildRtrax`

- An R package for ecologists and advanced users who work with environmental sensors
  
- It contains functions designed to meet most needs in order to organize, analyze and standardize data with the WildTrax infrastructure. 

## Why did you build `wildRtrax`?

- `wildRtrax` has been built in parallel with WildTrax to provide additional analytics and functionalities

- By outlining a standardized and harmonized procedure for data intake, quality control, processing and verification of environmental sensor data,`wildRtrax` and WildTrax hope to provide open workflows for using environmental sensors to help answer biological and ecological questions

---

## Who is `wildRtrax`?

- Us

- And YOU! (you'll learn how to make a pull request soon)

---

# Today's Agenda

- Installing the package
- Pre-processing acoustic data
- Downloading data from WildTrax
- Wrangling camera and acoustic data for analysis

---

Photo page here

---

# Installing the package

`remotes::install_github('ABbiodiversity/wildRtrax')`

Interested in recent fixes? Download the development branch instead:

`remotes::install_github('ABbiodiversity/wildRtrax@development')`

---

class: my-one-page-font

# Scanning acoustic data

```{r, echo=F, eval=T, warning=F, message=F}
load("webinar.RData")

```

```{r, echo=T, eval=F, warning=F, message=F}
# Load the package
library(wildRtrax)

# Plan futures
plan(strategy = multisession)

# Scan data
files <- wt_audio_scanner(path = ".", file_type = "wav", extra_cols = T)
```

```{r, echo=F, eval=T}
files

```

---

# Scanning acoustic data

```{r}
files |>
  names()

```

- `location` (where the recording was taken)
- `recording_date_time` (when the recording was taken)

**`339-NW_20230528_071000.wav`**

You can also add arguments `extra_cols` (sample_rate, length_seconds, n_channels) or `tz` if you want to assign a timezone to the recordings.

`wt_audio_scanner(path = '.', file_type = 'wav', extra_cols = T, tz = 'US/Mountain')`

---

# Filtering files

```{r, echo=T, eval=T, message=F, warning=F}
files |>
  mutate(hour = lubridate::hour(recording_date_time)) |>
  filter(julian %in% c(140:150),
         hour %in% c(4:8))

```

---

class: my-one-page-font

# Generating acoustic indices and LDFCs

```{r, echo=T, eval=F, warning=F, message=F}
# Use the files tibble to execute AP on them
wt_run_ap(x = files, output_dir = 'ap_outputs', path_to_ap = '/where/you/store/AP')

```

```{r, echo=T, eval=F, warning=F, message=F}
results <- wt_glean_ap(files |> 
              mutate(hour = lubridate::hour(recording_date_time)) |>
              filter(between(julian,110,220),
                     hour %in% c(0:3,22:23)), 
            input_dir = ".../ap_outputs", purpose = "biotic")


# The raw data
# results[[1]]

# The indices
results[[2]]

# The LDFC
results[[3]]

```

.pull-left[
<img src="339-nw-indices.png">
]

.pull-right[
<img src="339-nw.png">
]

---

# Creating tasks and tags

## Upload recordings and generate tasks simulatenously

+ Go to a Project and select Manage > Upload Recordings to Project

```{r, eval=F, echo=T, message=F, warning=F}
# Choose recordings for the ABMI Stratified Design for Ecosystem Health
tasks <- files |>
  inner_join(abmi_blocks, by = c("julian" = "julian", "time_index" = "time_index")) |>
  drop_na(blocks) |>
  group_by(location, blocks) |>
  sample_n(1, replace = F) |>
  ungroup() |>
  map(.x = .$file_path, .f = ~file.copy(.x, to = "/my/selected/files"))
```

---

# Creating tasks and tags

## Upload recordings and create tasks later

+ Go to the Organization > Recordings > Manage > Upload Recordings and create tasks later using `wt_make_aru_tasks()`

```{r, eval=F, echo=T, message=F, warning=F}
my_tasks <- wt_make_aru_tasks(
  tasks %>% 
    select(-c(blocks:recs)),
  output = NULL,
  task_method = "1SPT",
  task_length = 180
)

my_tasks
```

---

# Creating tasks and tags

.left-column[

## From Songscope

```{r, eval=F, echo=T, warning=F, message=F}
# Generate a tag csv to upload to WildTrax 
wt_songscope_tags(
  input,
  output = "env",
  my_output_file = NULL,
  species_code = "CONI",
  vocalization_type = "CALL",
  score_filter = 50,
  method = "USPM",
  task_length = 180
)

```
]

.right-column[

## From Kaleidoscope

```{r, eval=F, echo=T, warning=F, message=F}
# Generate a tag csv to upload to WildTrax 
wt_kaleidoscope_tags(input = files,
                     output = "my_bat_tags.csv"
                     freq_bump = 20000)

```

]
---

# Another day perhaps...

Other things to try (that we didn't have time for today)

- `wt_signal_level()` detects signals in audio based on amplitude thresholds
- `wt_chop()` divides a large audio file into shorter segments
- `wt_location_distances()` takes input latitude and longitudes and computes the distances between each set of valid points

---

## Handing it off to Marcus!

<img src="handingitoff.jpg" width=30% align=middle>
---



